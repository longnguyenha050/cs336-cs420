{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10319043,"sourceType":"datasetVersion","datasetId":6388699},{"sourceId":213326,"sourceType":"modelInstanceVersion","modelInstanceId":181835,"modelId":204070}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/ABaldrati/CLIP4Cir","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-29T09:43:06.973816Z","iopub.execute_input":"2024-12-29T09:43:06.974258Z","iopub.status.idle":"2024-12-29T09:43:07.608125Z","shell.execute_reply.started":"2024-12-29T09:43:06.974217Z","shell.execute_reply":"2024-12-29T09:43:07.607051Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'CLIP4Cir'...\nremote: Enumerating objects: 53, done.\u001b[K\nremote: Counting objects: 100% (22/22), done.\u001b[K\nremote: Compressing objects: 100% (16/16), done.\u001b[K\nremote: Total 53 (delta 14), reused 6 (delta 6), pack-reused 31 (from 1)\u001b[K\nReceiving objects: 100% (53/53), 1.52 MiB | 20.75 MiB/s, done.\nResolving deltas: 100% (18/18), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install comet-ml==3.21.0\n!pip install git+https://github.com/openai/CLIP.git\n!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f 'https://download.pytorch.org/whl/torch_stable.html'\n!pip install pandas==1.4.2\n!pip install urllib3==1.26.15\n!pip install -U comet-ml\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T16:33:19.926089Z","iopub.execute_input":"2024-12-28T16:33:19.926358Z","iopub.status.idle":"2024-12-28T16:33:26.080432Z","shell.execute_reply.started":"2024-12-28T16:33:19.926326Z","shell.execute_reply":"2024-12-28T16:33:26.079369Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://download.pytorch.org/whl/torch_stable.html\nRequirement already satisfied: torch==1.11.0+cu113 in /usr/local/lib/python3.10/dist-packages (1.11.0+cu113)\nRequirement already satisfied: torchvision==0.12.0+cu113 in /usr/local/lib/python3.10/dist-packages (0.12.0+cu113)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu113) (4.12.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (10.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2024.8.30)\nRequirement already satisfied: pandas==1.4.2 in /usr/local/lib/python3.10/dist-packages (1.4.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.2) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.2) (2024.2)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.2) (1.26.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.4.2) (1.16.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define source and destination directories\nsource_dir = '/kaggle/input/fashioniq2'\ndestination_dir = '/kaggle/working/CLIP4Cir'\n\n# Define the specific source directories for each folder\ndirectories_to_copy = {\n    'captions': 'captions-20220326T130604Z-001',\n    'image_splits': 'image_splits-20220326T130551Z-001',\n    'images': 'images'\n}\n\n# Loop through each directory and copy it\nfor folder_name, src_subdir in directories_to_copy.items():\n    src_path = os.path.join(source_dir, src_subdir)\n    dest_path = os.path.join(destination_dir, folder_name)\n    \n    # Ensure the destination directory exists\n    os.makedirs(dest_path, exist_ok=True)\n    \n    # Copy the directory content recursively\n    shutil.copytree(src_path, dest_path, dirs_exist_ok=True)\n\nprint(\"Directories copied successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T09:47:57.226241Z","iopub.execute_input":"2024-12-29T09:47:57.226624Z","iopub.status.idle":"2024-12-29T09:53:46.886113Z","shell.execute_reply.started":"2024-12-29T09:47:57.226601Z","shell.execute_reply":"2024-12-29T09:53:46.885178Z"}},"outputs":[{"name":"stdout","text":"Directories copied successfully.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define source and destination directories\nsource_dir = '/kaggle/working/CLIP4Cir'\ndestination_dir = os.path.join(source_dir, 'fashionIQ_dataset')\n\n# Create FashionIQ_dataset directory if it does not exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Define the directories to move\ndirectories_to_move = {\n    'captions/captions': 'captions',\n    'image_splits/image_splits': 'image_splits',\n    'images/images': 'images'\n}\n\n# Loop through each directory and check if it exists before moving\nfor src_subdir, dest_subdir in directories_to_move.items():\n    src_path = os.path.join(source_dir, src_subdir)\n    dest_path = os.path.join(destination_dir, dest_subdir)\n    \n    # Check if the source directory exists\n    if os.path.exists(src_path):\n        # Move the directory to the new location\n        shutil.move(src_path, dest_path)\n        print(f\"Moved {src_path} to {dest_path}\")\n    else:\n        print(f\"Source directory {src_path} does not exist.\")\n\nprint(\"Directories moved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T09:54:19.443516Z","iopub.execute_input":"2024-12-29T09:54:19.443810Z","iopub.status.idle":"2024-12-29T09:54:19.450746Z","shell.execute_reply.started":"2024-12-29T09:54:19.443788Z","shell.execute_reply":"2024-12-29T09:54:19.450137Z"}},"outputs":[{"name":"stdout","text":"Moved /kaggle/working/CLIP4Cir/captions/captions to /kaggle/working/CLIP4Cir/fashionIQ_dataset/captions\nMoved /kaggle/working/CLIP4Cir/image_splits/image_splits to /kaggle/working/CLIP4Cir/fashionIQ_dataset/image_splits\nMoved /kaggle/working/CLIP4Cir/images/images to /kaggle/working/CLIP4Cir/fashionIQ_dataset/images\nDirectories moved successfully.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from PIL import Image\nimport os\n\n# Define the directory containing the images\nimages_dir = '/kaggle/working/CLIP4Cir/fashionIQ_dataset/images'\n\n# Loop through all files in the directory\nfor filename in os.listdir(images_dir):\n    if filename.endswith('.png'):  # Check if the file is a PNG image\n        png_path = os.path.join(images_dir, filename)\n        \n        # Open the PNG image\n        with Image.open(png_path) as img:\n            # Convert the image to RGB (required for saving as JPG)\n            rgb_img = img.convert('RGB')\n            \n            # Define the new file path with JPG extension\n            jpg_path = os.path.join(images_dir, filename.replace('.png', '.jpg'))\n            \n            # Save the image as JPG\n            rgb_img.save(jpg_path, 'JPEG')\n\n            # Optionally, delete the original PNG file\n            os.remove(png_path)\n\nprint(\"PNG files have been successfully converted to JPG.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T09:54:38.497019Z","iopub.execute_input":"2024-12-29T09:54:38.497299Z","iopub.status.idle":"2024-12-29T09:57:14.670368Z","shell.execute_reply.started":"2024-12-29T09:54:38.497270Z","shell.execute_reply":"2024-12-29T09:57:14.669619Z"}},"outputs":[{"name":"stdout","text":"PNG files have been successfully converted to JPG.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T09:59:01.443853Z","iopub.execute_input":"2024-12-29T09:59:01.444206Z","iopub.status.idle":"2024-12-29T09:59:04.355710Z","shell.execute_reply.started":"2024-12-29T09:59:01.444182Z","shell.execute_reply":"2024-12-29T09:59:04.355052Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from comet_ml import Experiment\nimport json\nimport multiprocessing\nfrom argparse import ArgumentParser\nfrom datetime import datetime\nfrom pathlib import Path\nfrom statistics import mean, geometric_mean, harmonic_mean\nfrom typing import List\nimport clip\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch import optim, nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom data_utils import base_path, squarepad_transform, targetpad_transform, CIRRDataset, FashionIQDataset\nfrom utils import collate_fn, update_train_running_results, set_train_bar_description, extract_index_features, \\\n    save_model, generate_randomized_fiq_caption, element_wise_sum, device\nfrom validate import compute_cirr_val_metrics, compute_fiq_val_metrics\n\n\ndef clip_finetune_fiq(train_dress_types: List[str], val_dress_types: List[str],\n                      num_epochs: int, clip_model_name: str, learning_rate: float, batch_size: int,\n                      validation_frequency: int, transform: str, save_training: bool, encoder: str, save_best: bool,\n                      **kwargs):\n    \"\"\"\n    Fine-tune CLIP on the FashionIQ dataset using as combining function the image-text element-wise sum\n    :param train_dress_types: FashionIQ categories to train on\n    :param val_dress_types: FashionIQ categories to validate on\n    :param num_epochs: number of epochs\n    :param clip_model_name: CLIP model you want to use: \"RN50\", \"RN101\", \"RN50x4\"...\n    :param learning_rate: fine-tuning leanring rate\n    :param batch_size: batch size\n    :param validation_frequency: validation frequency expressed in epoch\n    :param transform: preprocess transform you want to use. Should be in ['clip', 'squarepad', 'targetpad']. When\n                targetpad is also required to provide `target_ratio` kwarg.\n    :param save_training: when True save the weights of the fine-tuned CLIP model\n    :param encoder: which CLIP encoder to fine-tune, should be in ['both', 'text', 'image']\n    :param save_best: when True save only the weights of the best CLIP model wrt the average_recall metric\n    :param kwargs: if you use the `targetpad` transform you should prove `target_ratio` as kwarg\n    \"\"\"\n\n    training_start = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n    training_path: Path = Path(\n        base_path / f\"models/clip_finetuned_on_fiq_{clip_model_name}_{training_start}\")\n    training_path.mkdir(exist_ok=False, parents=True)\n\n    saved_model_path = '/kaggle/input/clip/other/default/1/tuned_clip_best.pt'\n    \n    # Save all the hyperparameters on a file\n    with open(training_path / \"training_hyperparameters.json\", 'w+') as file:\n        json.dump(training_hyper_params, file, sort_keys=True, indent=4)\n\n    clip_model, clip_preprocess = clip.load(clip_model_name, device=device, jit=False)\n    \n    if saved_model_path:\n        print(f\"Loading pre-trained model from {saved_model_path}\")\n        clip_model.load_state_dict(torch.load(saved_model_path), strict=False)\n\n    if encoder == 'text':\n        print('Only the CLIP text encoder will be fine-tuned')\n        for param in clip_model.visual.parameters():\n            param.requires_grad = False\n    elif encoder == 'image':\n        print('Only the CLIP image encoder will be fine-tuned')\n        for param in clip_model.parameters():\n            param.requires_grad = False\n        for param in clip_model.visual.parameters():\n            param.requires_grad = True\n    elif encoder == 'both':\n        print('Both CLIP encoders will be fine-tuned')\n    else:\n        raise ValueError(\"encoder parameter should be in ['text', 'image', both']\")\n\n    clip_model.eval().float()\n    input_dim = clip_model.visual.input_resolution\n\n    if transform == \"clip\":\n        preprocess = clip_preprocess\n        print('CLIP default preprocess pipeline is used')\n    elif transform == \"squarepad\":\n        preprocess = squarepad_transform(input_dim)\n        print('Square pad preprocess pipeline is used')\n    elif transform == \"targetpad\":\n        target_ratio = kwargs['target_ratio']\n        preprocess = targetpad_transform(target_ratio, input_dim)\n        print(f'Target pad with {target_ratio = } preprocess pipeline is used')\n    else:\n        raise ValueError(\"Preprocess transform should be in ['clip', 'squarepad', 'targetpad']\")\n\n    idx_to_dress_mapping = {}\n    relative_val_datasets = []\n    classic_val_datasets = []\n\n    # When fine-tuning only the text encoder we can precompute the index features since they do not change over\n    # the epochs\n    if encoder == 'text':\n        index_features_list = []\n        index_names_list = []\n\n    # Define the validation datasets\n    for idx, dress_type in enumerate(val_dress_types):\n        idx_to_dress_mapping[idx] = dress_type\n        relative_val_dataset = FashionIQDataset('val', [dress_type], 'relative', preprocess, )\n        relative_val_datasets.append(relative_val_dataset)\n        classic_val_dataset = FashionIQDataset('val', [dress_type], 'classic', preprocess, )\n        classic_val_datasets.append(classic_val_dataset)\n        if encoder == 'text':\n            index_features_and_names = extract_index_features(classic_val_dataset, clip_model)\n            index_features_list.append(index_features_and_names[0])\n            index_names_list.append(index_features_and_names[1])\n\n    # Define the train datasets and the combining function\n    relative_train_dataset = FashionIQDataset('train', train_dress_types, 'relative', preprocess)\n    relative_train_loader = DataLoader(dataset=relative_train_dataset, batch_size=batch_size,\n                                       num_workers=multiprocessing.cpu_count(), pin_memory=False, collate_fn=collate_fn,\n                                       drop_last=True, shuffle=True)\n    combining_function = element_wise_sum\n\n    # Define the optimizer, the loss and the grad scaler\n    optimizer = optim.AdamW(\n        [{'params': filter(lambda p: p.requires_grad, clip_model.parameters()), 'lr': learning_rate,\n          'betas': (0.9, 0.999), 'eps': 1e-7}])\n    if saved_optimizer_path:\n        print(f\"Loading optimizer state from {saved_optimizer_path}\")\n        optimizer.load_state_dict(torch.load(saved_optimizer_path))\n\n    # Load grad scaler state if saved\n    scaler = torch.cuda.amp.GradScaler()\n    if saved_scaler_path:\n        print(f\"Loading scaler state from {saved_scaler_path}\")\n        scaler.load_state_dict(torch.load(saved_scaler_path))\n\n    # Continue training for the remaining epochs\n    start_epoch = 40  # Assuming the previous fine-tuning ran for 40 epochs\n    end_epoch = start_epoch + num_epochs\n    print(f\"Continuing training from epoch {start_epoch + 1} to epoch {end_epoch}\")\n    \n    # When save_best == True initialize the best result to zero\n    if save_best:\n        best_avg_recall = 0\n\n    # Define dataframes for CSV logging\n    training_log_frame = pd.DataFrame()\n    validation_log_frame = pd.DataFrame()\n\n    # Start with the training loop\n    print('Training loop started')\n     for epoch in range(start_epoch, end_epoch):\n        with experiment.train():\n            train_running_results = {'images_in_epoch': 0, 'accumulated_train_loss': 0}\n            train_bar = tqdm(relative_train_loader, ncols=150)\n            for idx, (reference_images, target_images, captions) in enumerate(train_bar):\n                images_in_batch = reference_images.size(0)\n                step = len(train_bar) * epoch + idx\n\n                optimizer.zero_grad()\n\n                reference_images = reference_images.to(device, non_blocking=True)\n                target_images = target_images.to(device, non_blocking=True)\n\n                # Randomize the training caption\n                flattened_captions: list = np.array(captions).T.flatten().tolist()\n                captions = generate_randomized_fiq_caption(flattened_captions)\n                text_inputs = clip.tokenize(captions, context_length=77, truncate=True).to(device, non_blocking=True)\n\n                # Extract features, compute logits, and loss\n                with torch.cuda.amp.autocast():\n                    reference_features = clip_model.encode_image(reference_images)\n                    caption_features = clip_model.encode_text(text_inputs)\n                    predicted_features = combining_function(reference_features, caption_features)\n                    target_features = F.normalize(clip_model.encode_image(target_images))\n\n                    logits = 100 * predicted_features @ target_features.T\n\n                    ground_truth = torch.arange(images_in_batch, dtype=torch.long, device=device)\n                    loss = crossentropy_criterion(logits, ground_truth)\n\n                # Backpropagate and update weights\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n\n                experiment.log_metric('step_loss', loss.detach().cpu().item(), step=step)\n                update_train_running_results(train_running_results, loss, images_in_batch)\n                set_train_bar_description(train_bar, epoch, end_epoch, train_running_results)\n\n            # Save epoch loss and log metrics\n            train_epoch_loss = float(\n                train_running_results['accumulated_train_loss'] / train_running_results['images_in_epoch'])\n            experiment.log_metric('epoch_loss', train_epoch_loss, epoch=epoch)\n\n            # Training CSV logging\n            training_log_frame = pd.concat(\n                [training_log_frame,\n                 pd.DataFrame(data={'epoch': epoch, 'train_epoch_loss': train_epoch_loss}, index=[0])])\n            training_log_frame.to_csv(str(training_path / 'train_metrics.csv'), index=False)\n\n        if epoch % validation_frequency == 0:\n            with experiment.validate():\n                recalls_at10 = []\n                recalls_at50 = []\n\n                # Compute and log validation metrics for each validation dataset (which corresponds to a different\n                # FashionIQ category)\n                for relative_val_dataset, classic_val_dataset, idx in zip(relative_val_datasets, classic_val_datasets,\n                                                                          idx_to_dress_mapping):\n                    if encoder == 'text':\n                        index_features, index_names = index_features_list[idx], index_names_list[idx]\n                    else:\n                        index_features, index_names = extract_index_features(classic_val_dataset, clip_model)\n                    recall_at10, recall_at50 = compute_fiq_val_metrics(relative_val_dataset, clip_model,\n                                                                       index_features, index_names,\n                                                                       combining_function)\n                    recalls_at10.append(recall_at10)\n                    recalls_at50.append(recall_at50)\n\n                results_dict = {}\n                for i in range(len(recalls_at10)):\n                    results_dict[f'{idx_to_dress_mapping[i]}_recall_at10'] = recalls_at10[i]\n                    results_dict[f'{idx_to_dress_mapping[i]}_recall_at50'] = recalls_at50[i]\n                results_dict.update({\n                    f'average_recall_at10': mean(recalls_at10),\n                    f'average_recall_at50': mean(recalls_at50),\n                    f'average_recall': (mean(recalls_at50) + mean(recalls_at10)) / 2\n                })\n\n                print(json.dumps(results_dict, indent=4))\n                experiment.log_metrics(\n                    results_dict,\n                    epoch=epoch\n                )\n\n                # Validation CSV logging\n                log_dict = {'epoch': epoch}\n                log_dict.update(results_dict)\n                validation_log_frame = pd.concat([validation_log_frame, pd.DataFrame(data=log_dict, index=[0])])\n                validation_log_frame.to_csv(str(training_path / 'validation_metrics.csv'), index=False)\n\n            if save_training:\n                if save_best and results_dict['average_recall'] > best_avg_recall:\n                    best_avg_recall = results_dict['average_recall']\n                    save_model('tuned_clip_best', epoch, clip_model, training_path)\n                elif not save_best:\n                    save_model(f'tuned_clip_{epoch}', epoch, clip_model, training_path)\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument(\"--dataset\", type=str, required=True, help=\"fashionIQ'\")\n    parser.add_argument(\"--api-key\", type=str, help=\"api for Comet logging\")\n    parser.add_argument(\"--workspace\", type=str, help=\"workspace of Comet logging\")\n    parser.add_argument(\"--experiment-name\", type=str, help=\"name of the experiment on Comet\")\n    parser.add_argument(\"--num-epochs\", default=300, type=int, help=\"number training epochs\")\n    parser.add_argument(\"--clip-model-name\", default=\"RN50x4\", type=str, help=\"CLIP model to use, e.g 'RN50', 'RN50x4'\")\n    parser.add_argument(\"--encoder\", default='both', type=str,\n                        help=\"Which CLIP encoder to fine-tune, should be in ['both', 'text', 'image']\")\n    parser.add_argument(\"--learning-rate\", default=2e-6, type=float, help=\"Learning rate\")\n    parser.add_argument(\"--batch-size\", default=512, type=int, help=\"Batch size\")\n    parser.add_argument(\"--validation-frequency\", default=1, type=int, help=\"Validation frequency expressed in epochs\")\n    parser.add_argument(\"--target-ratio\", default=1.25, type=float, help=\"TargetPad target ratio\")\n    parser.add_argument(\"--transform\", default=\"targetpad\", type=str,\n                        help=\"Preprocess pipeline, should be in ['clip', 'squarepad', 'targetpad'] \")\n    parser.add_argument(\"--save-training\", dest=\"save_training\", action='store_true',\n                        help=\"Whether save the training model\")\n    parser.add_argument(\"--save-best\", dest=\"save_best\", action='store_true',\n                        help=\"Save only the best model during training\")\n\n    args = parser.parse_args()\n    \n    training_hyper_params = {\n        \"num_epochs\": args.num_epochs,\n        \"clip_model_name\": args.clip_model_name,\n        \"learning_rate\": args.learning_rate,\n        \"batch_size\": args.batch_size,\n        \"validation_frequency\": args.validation_frequency,\n        \"transform\": args.transform,\n        \"target_ratio\": args.target_ratio,\n        \"save_training\": args.save_training,\n        \"encoder\": args.encoder,\n        \"save_best\": args.save_best\n    }\n\n    if args.api_key and args.workspace:\n        print(\"Comet logging ENABLED\")\n        experiment = Experiment(\n            api_key=args.api_key,\n            project_name=f\"{args.dataset} clip fine-tuning\",\n            workspace=args.workspace,\n            disabled=False\n        )\n        if args.experiment_name:\n            experiment.set_name(args.experiment_name)\n    else:\n        print(\"Comet loging DISABLED, in order to enable it you need to provide an api key and a workspace\")\n        experiment = Experiment(\n            api_key=\"\",\n            project_name=\"\",\n            workspace=\"\",\n            disabled=True\n        )\n\n    experiment.log_code(folder=str(base_path / 'src'))\n    experiment.log_parameters(training_hyper_params)\n\n    elif args.dataset.lower() == 'fashioniq':\n        training_hyper_params.update(\n            {'train_dress_types': ['dress', 'toptee', 'shirt'], 'val_dress_types': ['dress', 'toptee', 'shirt']})\n        clip_finetune_fiq(**training_hyper_params) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python /kaggle/working/CLIP4Cir/src/clip_fine_tune.py \\\n   --dataset FashionIQ \\\n   --api-key bKMwHTWGZ1fqpCdlAWHrPrP1M \\\n   --workspace longnguyenha050 \\\n   --experiment-name clip-finetune-fashioniq \\\n   --num-epochs 60 \\\n   --clip-model-name RN50x4 \\\n   --encoder both \\\n   --learning-rate 2e-6 \\\n   --batch-size 32 \\\n   --transform targetpad \\\n   --target-ratio 1.25  \\\n   --save-training \\\n   --save-best \\\n   --validation-frequency 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T10:59:39.799184Z","iopub.execute_input":"2024-12-29T10:59:39.799522Z","iopub.status.idle":"2024-12-29T13:18:23.752734Z","shell.execute_reply.started":"2024-12-29T10:59:39.799493Z","shell.execute_reply":"2024-12-29T13:18:23.751596Z"}},"outputs":[{"name":"stdout","text":"Comet logging ENABLED\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/longnguyenha050/fashioniq-clip-fine-tuning/62e69cf7e1af4d4f97246176a269c5f6\u001b[0m\n\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\nLoading pre-trained model from /kaggle/input/clip/other/default/1/tuned_clip_best.pt\n/kaggle/working/CLIP4Cir/src/clip_fine_tune.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  clip_model.load_state_dict(torch.load(saved_model_path), strict=False)\nBoth CLIP encoders will be fine-tuned\nTarget pad with target_ratio = 1.25 preprocess pipeline is used\nFashionIQ val - ['dress'] dataset in relative mode initialized\nFashionIQ val - ['dress'] dataset in classic mode initialized\nFashionIQ val - ['toptee'] dataset in relative mode initialized\nFashionIQ val - ['toptee'] dataset in classic mode initialized\nFashionIQ val - ['shirt'] dataset in relative mode initialized\nFashionIQ val - ['shirt'] dataset in classic mode initialized\nFashionIQ train - ['dress', 'toptee', 'shirt'] dataset in relative mode initialized\n/kaggle/working/CLIP4Cir/src/clip_fine_tune.py:127: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\nContinuing training from epoch 41 to epoch 100\nTraining loop started\n  0%|                                                                                                                         | 0/562 [00:00<?, ?it/s]/kaggle/working/CLIP4Cir/src/clip_fine_tune.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n[40/100] train loss: 0.852 : 100%|██████████████████████████████████████████████████████████████████████████████████| 562/562 [11:20<00:00,  1.21s/it]\nextracting fashionIQ ['dress'] - val index features\n100%|█████████████████████████████████████████| 120/120 [00:53<00:00,  2.26it/s]\nCompute FashionIQ ['dress'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.86it/s]\nCompute FashionIQ ['dress'] validation metrics\nextracting fashionIQ ['toptee'] - val index features\n100%|█████████████████████████████████████████| 168/168 [01:14<00:00,  2.26it/s]\nCompute FashionIQ ['toptee'] validation predictions\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.60it/s]\nCompute FashionIQ ['toptee'] validation metrics\nextracting fashionIQ ['shirt'] - val index features\n100%|█████████████████████████████████████████| 199/199 [01:27<00:00,  2.26it/s]\nCompute FashionIQ ['shirt'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.80it/s]\nCompute FashionIQ ['shirt'] validation metrics\n{\n    \"dress_recall_at10\": 32.57312774658203,\n    \"dress_recall_at50\": 57.015371322631836,\n    \"toptee_recall_at10\": 41.25446081161499,\n    \"toptee_recall_at50\": 65.06884098052979,\n    \"shirt_recall_at10\": 38.420018553733826,\n    \"shirt_recall_at50\": 59.81354117393494,\n    \"average_recall_at10\": 37.41586903731028,\n    \"average_recall_at50\": 60.63258449236552,\n    \"average_recall\": 49.0242267648379\n}\n[41/100] train loss: 0.627 : 100%|██████████████████████████████████████████████████████████████████████████████████| 562/562 [11:19<00:00,  1.21s/it]\nextracting fashionIQ ['dress'] - val index features\n100%|█████████████████████████████████████████| 120/120 [00:52<00:00,  2.28it/s]\nCompute FashionIQ ['dress'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.86it/s]\nCompute FashionIQ ['dress'] validation metrics\nextracting fashionIQ ['toptee'] - val index features\n100%|█████████████████████████████████████████| 168/168 [01:14<00:00,  2.26it/s]\nCompute FashionIQ ['toptee'] validation predictions\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.62it/s]\nCompute FashionIQ ['toptee'] validation metrics\nextracting fashionIQ ['shirt'] - val index features\n100%|█████████████████████████████████████████| 199/199 [01:27<00:00,  2.27it/s]\nCompute FashionIQ ['shirt'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.84it/s]\nCompute FashionIQ ['shirt'] validation metrics\n{\n    \"dress_recall_at10\": 34.35795605182648,\n    \"dress_recall_at50\": 59.1968297958374,\n    \"toptee_recall_at10\": 41.81540012359619,\n    \"toptee_recall_at50\": 66.13972187042236,\n    \"shirt_recall_at10\": 39.49950933456421,\n    \"shirt_recall_at50\": 61.678117513656616,\n    \"average_recall_at10\": 38.55762183666229,\n    \"average_recall_at50\": 62.33822305997213,\n    \"average_recall\": 50.447922448317215\n}\n[42/100] train loss: 0.534 : 100%|██████████████████████████████████████████████████████████████████████████████████| 562/562 [11:19<00:00,  1.21s/it]\nextracting fashionIQ ['dress'] - val index features\n100%|█████████████████████████████████████████| 120/120 [00:52<00:00,  2.28it/s]\nCompute FashionIQ ['dress'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.84it/s]\nCompute FashionIQ ['dress'] validation metrics\nextracting fashionIQ ['toptee'] - val index features\n100%|█████████████████████████████████████████| 168/168 [01:14<00:00,  2.26it/s]\nCompute FashionIQ ['toptee'] validation predictions\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.58it/s]\nCompute FashionIQ ['toptee'] validation metrics\nextracting fashionIQ ['shirt'] - val index features\n100%|█████████████████████████████████████████| 199/199 [01:27<00:00,  2.26it/s]\nCompute FashionIQ ['shirt'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.83it/s]\nCompute FashionIQ ['shirt'] validation metrics\n{\n    \"dress_recall_at10\": 33.961328864097595,\n    \"dress_recall_at50\": 59.295982122421265,\n    \"toptee_recall_at10\": 42.68230497837067,\n    \"toptee_recall_at50\": 65.62978029251099,\n    \"shirt_recall_at10\": 39.69578146934509,\n    \"shirt_recall_at50\": 60.84396243095398,\n    \"average_recall_at10\": 38.77980510393778,\n    \"average_recall_at50\": 61.92324161529541,\n    \"average_recall\": 50.35152335961659\n}\n[43/100] train loss: 0.485 : 100%|██████████████████████████████████████████████████████████████████████████████████| 562/562 [11:19<00:00,  1.21s/it]\nextracting fashionIQ ['dress'] - val index features\n100%|█████████████████████████████████████████| 120/120 [00:52<00:00,  2.29it/s]\nCompute FashionIQ ['dress'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.90it/s]\nCompute FashionIQ ['dress'] validation metrics\nextracting fashionIQ ['toptee'] - val index features\n100%|█████████████████████████████████████████| 168/168 [01:14<00:00,  2.27it/s]\nCompute FashionIQ ['toptee'] validation predictions\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.63it/s]\nCompute FashionIQ ['toptee'] validation metrics\nextracting fashionIQ ['shirt'] - val index features\n100%|█████████████████████████████████████████| 199/199 [01:27<00:00,  2.26it/s]\nCompute FashionIQ ['shirt'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.82it/s]\nCompute FashionIQ ['shirt'] validation metrics\n{\n    \"dress_recall_at10\": 34.90332067012787,\n    \"dress_recall_at50\": 60.43629050254822,\n    \"toptee_recall_at10\": 42.88628399372101,\n    \"toptee_recall_at50\": 67.21060872077942,\n    \"shirt_recall_at10\": 39.54857587814331,\n    \"shirt_recall_at50\": 61.678117513656616,\n    \"average_recall_at10\": 39.11272684733073,\n    \"average_recall_at50\": 63.10833891232809,\n    \"average_recall\": 51.11053287982941\n}\n[44/100] train loss: 0.416 : 100%|██████████████████████████████████████████████████████████████████████████████████| 562/562 [11:18<00:00,  1.21s/it]\nextracting fashionIQ ['dress'] - val index features\n100%|█████████████████████████████████████████| 120/120 [00:52<00:00,  2.28it/s]\nCompute FashionIQ ['dress'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.88it/s]\nCompute FashionIQ ['dress'] validation metrics\nextracting fashionIQ ['toptee'] - val index features\n100%|█████████████████████████████████████████| 168/168 [01:14<00:00,  2.27it/s]\nCompute FashionIQ ['toptee'] validation predictions\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.63it/s]\nCompute FashionIQ ['toptee'] validation metrics\nextracting fashionIQ ['shirt'] - val index features\n100%|█████████████████████████████████████████| 199/199 [01:27<00:00,  2.27it/s]\nCompute FashionIQ ['shirt'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.80it/s]\nCompute FashionIQ ['shirt'] validation metrics\n{\n    \"dress_recall_at10\": 34.209221601486206,\n    \"dress_recall_at50\": 60.68418622016907,\n    \"toptee_recall_at10\": 42.37633943557739,\n    \"toptee_recall_at50\": 67.05762147903442,\n    \"shirt_recall_at10\": 40.38272798061371,\n    \"shirt_recall_at50\": 62.61039972305298,\n    \"average_recall_at10\": 38.989429672559105,\n    \"average_recall_at50\": 63.45073580741882,\n    \"average_recall\": 51.22008273998897\n}\n[45/100] train loss: 0.379 : 100%|██████████████████████████████████████████████████████████████████████████████████| 562/562 [11:19<00:00,  1.21s/it]\nextracting fashionIQ ['dress'] - val index features\n100%|█████████████████████████████████████████| 120/120 [00:52<00:00,  2.29it/s]\nCompute FashionIQ ['dress'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.86it/s]\nCompute FashionIQ ['dress'] validation metrics\nextracting fashionIQ ['toptee'] - val index features\n100%|█████████████████████████████████████████| 168/168 [01:14<00:00,  2.26it/s]\nCompute FashionIQ ['toptee'] validation predictions\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.59it/s]\nCompute FashionIQ ['toptee'] validation metrics\nextracting fashionIQ ['shirt'] - val index features\n100%|█████████████████████████████████████████| 199/199 [01:27<00:00,  2.26it/s]\nCompute FashionIQ ['shirt'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.81it/s]\nCompute FashionIQ ['shirt'] validation metrics\n{\n    \"dress_recall_at10\": 34.7545862197876,\n    \"dress_recall_at50\": 60.98165512084961,\n    \"toptee_recall_at10\": 42.98827052116394,\n    \"toptee_recall_at50\": 67.97552108764648,\n    \"shirt_recall_at10\": 40.08832275867462,\n    \"shirt_recall_at50\": 61.9234561920166,\n    \"average_recall_at10\": 39.27705983320872,\n    \"average_recall_at50\": 63.62687746683756,\n    \"average_recall\": 51.45196865002314\n}\n[46/100] train loss: 0.351 : 100%|██████████████████████████████████████████████████████████████████████████████████| 562/562 [11:18<00:00,  1.21s/it]\nextracting fashionIQ ['dress'] - val index features\n100%|█████████████████████████████████████████| 120/120 [00:52<00:00,  2.28it/s]\nCompute FashionIQ ['dress'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.96it/s]\nCompute FashionIQ ['dress'] validation metrics\nextracting fashionIQ ['toptee'] - val index features\n100%|█████████████████████████████████████████| 168/168 [01:14<00:00,  2.27it/s]\nCompute FashionIQ ['toptee'] validation predictions\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.63it/s]\nCompute FashionIQ ['toptee'] validation metrics\nextracting fashionIQ ['shirt'] - val index features\n100%|█████████████████████████████████████████| 199/199 [01:27<00:00,  2.27it/s]\nCompute FashionIQ ['shirt'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.85it/s]\nCompute FashionIQ ['shirt'] validation metrics\n{\n    \"dress_recall_at10\": 34.60584878921509,\n    \"dress_recall_at50\": 61.42786145210266,\n    \"toptee_recall_at10\": 43.1412547826767,\n    \"toptee_recall_at50\": 68.12850832939148,\n    \"shirt_recall_at10\": 38.76349329948425,\n    \"shirt_recall_at50\": 62.26692795753479,\n    \"average_recall_at10\": 38.836865623792015,\n    \"average_recall_at50\": 63.94109924634298,\n    \"average_recall\": 51.3889824350675\n}\n[47/100] train loss: 0.315 : 100%|██████████████████████████████████████████████████████████████████████████████████| 562/562 [11:20<00:00,  1.21s/it]\nextracting fashionIQ ['dress'] - val index features\n100%|█████████████████████████████████████████| 120/120 [00:52<00:00,  2.29it/s]\nCompute FashionIQ ['dress'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.90it/s]\nCompute FashionIQ ['dress'] validation metrics\nextracting fashionIQ ['toptee'] - val index features\n100%|█████████████████████████████████████████| 168/168 [01:14<00:00,  2.27it/s]\nCompute FashionIQ ['toptee'] validation predictions\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.54it/s]\nCompute FashionIQ ['toptee'] validation metrics\nextracting fashionIQ ['shirt'] - val index features\n100%|█████████████████████████████████████████| 199/199 [01:27<00:00,  2.26it/s]\nCompute FashionIQ ['shirt'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.79it/s]\nCompute FashionIQ ['shirt'] validation metrics\n{\n    \"dress_recall_at10\": 34.010908007621765,\n    \"dress_recall_at50\": 60.58502793312073,\n    \"toptee_recall_at10\": 42.17236042022705,\n    \"toptee_recall_at50\": 65.68077802658081,\n    \"shirt_recall_at10\": 40.235525369644165,\n    \"shirt_recall_at50\": 62.41413354873657,\n    \"average_recall_at10\": 38.80626459916433,\n    \"average_recall_at50\": 62.89331316947937,\n    \"average_recall\": 50.84978888432185\n}\n[48/100] train loss: 0.288 : 100%|██████████████████████████████████████████████████████████████████████████████████| 562/562 [11:20<00:00,  1.21s/it]\nextracting fashionIQ ['dress'] - val index features\n100%|█████████████████████████████████████████| 120/120 [00:52<00:00,  2.29it/s]\nCompute FashionIQ ['dress'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.89it/s]\nCompute FashionIQ ['dress'] validation metrics\nextracting fashionIQ ['toptee'] - val index features\n100%|█████████████████████████████████████████| 168/168 [01:14<00:00,  2.26it/s]\nCompute FashionIQ ['toptee'] validation predictions\n100%|███████████████████████████████████████████| 62/62 [00:08<00:00,  7.55it/s]\nCompute FashionIQ ['toptee'] validation metrics\nextracting fashionIQ ['shirt'] - val index features\n100%|█████████████████████████████████████████| 199/199 [01:28<00:00,  2.26it/s]\nCompute FashionIQ ['shirt'] validation predictions\n100%|███████████████████████████████████████████| 64/64 [00:08<00:00,  7.80it/s]\nCompute FashionIQ ['shirt'] validation metrics\n{\n    \"dress_recall_at10\": 35.20079255104065,\n    \"dress_recall_at50\": 60.138821601867676,\n    \"toptee_recall_at10\": 41.662415862083435,\n    \"toptee_recall_at50\": 65.9357488155365,\n    \"shirt_recall_at10\": 39.303237199783325,\n    \"shirt_recall_at50\": 62.16879487037659,\n    \"average_recall_at10\": 38.7221485376358,\n    \"average_recall_at50\": 62.747788429260254,\n    \"average_recall\": 50.73496848344803\n}\n[49/100] train loss: 0.145 :   1%|▋                                                                                   | 5/562 [00:06<11:53,  1.28s/it]^C\n[49/100] train loss: 0.145 :   1%|▋                                                                                   | 5/562 [00:07<13:26,  1.45s/it]\nTraceback (most recent call last):\n  File \"/kaggle/working/CLIP4Cir/src/clip_fine_tune.py\", line 302, in <module>\n    clip_finetune_fiq(**training_hyper_params) \n  File \"/kaggle/working/CLIP4Cir/src/clip_fine_tune.py\", line 164, in clip_finetune_fiq\n    caption_features = clip_model.encode_text(text_inputs)\n  File \"/usr/local/lib/python3.10/dist-packages/clip/model.py\", line 354, in encode_text\n","output_type":"stream"}],"execution_count":24}]}